{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "movies_dev = pd.read_excel('Dev_Set/dev_set_groundtruth_and_trailers.xls',usecols=['movie','goodforairplane','filename'],index_col='filename')\n",
    "movies_test = pd.read_excel('Test_set/dataset_complete.xlsx',names=['movie','filename','goodforairplane'],header=None, skiprows=1,index_col='filename',sep=';')\n",
    "movies = pd.concat([movies_dev,movies_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xml.etree.cElementTree as et\n",
    "\n",
    "for filename in os.listdir('Dev_Set/XML'):\n",
    "    tree = et.parse('Dev_Set/XML/'+filename)\n",
    "    root = tree.getroot()\n",
    "    tmp = root[0]\n",
    "    \n",
    "    for i in tmp.attrib:\n",
    "        movies.loc[filename[:-4],i] = tmp.get(i)\n",
    "        \n",
    "for filename in os.listdir('Test_Set/XML'):\n",
    "    tree = et.parse('Test_Set/XML/'+filename)\n",
    "    root = tree.getroot()\n",
    "    tmp = root[0]\n",
    "    for i in tmp.attrib:\n",
    "        movies.loc[filename[:-4],i] = tmp.get(i)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "movies.drop(['released','Website','imdbID','poster','tomatoConsensus','writer','DVD','plot','title','awards'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "movies = movies.replace('N/A','NaN')\n",
    "movies['imdbRating'] = movies['imdbRating'].astype(np.float)\n",
    "movies['imdbVotes'] = movies['imdbVotes'].replace('[,]','',regex=True).astype(float)\n",
    "movies['runtime'] = movies['runtime'].replace('[\\smin]','',regex=True).astype(float)\n",
    "movies['rated'] = movies['rated'].replace('NaN','NOT RATED')\n",
    "movies['year'] = movies['year'].astype(float)\n",
    "movies['metascore'] = movies['metascore'].astype(float)\n",
    "movies['tomatoRating'] = movies['tomatoRating'].astype(float)\n",
    "movies['tomatoUserRating'] = movies['tomatoUserRating'].astype(float)\n",
    "movies['tomatoMeter'] = movies['tomatoMeter'].astype(float)\n",
    "movies['tomatoUserMeter'] = movies['tomatoUserMeter'].astype(float)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "movies.fillna(0,inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "metadata = movies[['language','year','genre','country','runtime','rated']]\n",
    "userrating = movies[['imdbRating','metascore','tomatoRating','tomatoUserRating','tomatoMeter','tomatoUserMeter']]\n",
    "meta_and_user = movies[['language','year','genre','country','runtime','rated','imdbRating','metascore','tomatoRating',\n",
    "                       'tomatoUserRating','tomatoMeter','tomatoUserMeter']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import Encoding as enc\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    enc.one_hot_encode(metadata,'language',True)\n",
    "    enc.one_hot_encode(metadata,'genre',True)\n",
    "    enc.one_hot_encode(metadata, 'country',True)\n",
    "    enc.one_hot_encode(metadata, 'rated',True)\n",
    "\n",
    "metadata.drop(['language','country','rated','genre'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    enc.one_hot_encode(meta_and_user,'language',True)\n",
    "    enc.one_hot_encode(meta_and_user, 'country',True)\n",
    "    enc.one_hot_encode(meta_and_user, 'genre',True)\n",
    "    enc.one_hot_encode(meta_and_user, 'rated',True)\n",
    "\n",
    "meta_and_user.drop(['language','country','rated','genre'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski',\n",
    "                           metric_params=None,n_jobs=-1)\n",
    "nc = NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
    "tree = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                              min_weight_fraction_leaf=0.0, max_features=None, random_state=0, max_leaf_nodes=None,\n",
    "                              class_weight=None, presort=False)\n",
    "log = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1,\n",
    "                         class_weight=None, random_state=0, solver='liblinear', max_iter=100, multi_class='ovr', \n",
    "                         verbose=0, warm_start=False, n_jobs=-1)\n",
    "svm = SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, \n",
    "          cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=None, random_state=0)\n",
    "bag = BaggingClassifier(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, \n",
    "                        bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=-1, random_state=0, verbose=0)\n",
    "rf = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "                            min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, bootstrap=True, \n",
    "                            oob_score=False, n_jobs=-1, random_state=0, verbose=0, warm_start=False, class_weight=None)\n",
    "ada = AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=0)\n",
    "gb = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, min_samples_split=2,\n",
    "                                min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, init=None, random_state=0,\n",
    "                                max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto')\n",
    "gauss = GaussianNB()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Precision: 0.5956349206349205, Recall: 0.6366666666666666, f1: 0.5642607392607393\n",
      "Precision: 0.5595238095238094, Recall: 0.5366666666666666, f1: 0.5382478632478632\n",
      "Precision: 0.49000000000000005, Recall: 0.5666666666666668, f1: 0.5061033411033411\n",
      "Precision: 0.5125, Recall: 0.56, f1: 0.5244599844599845\n",
      "Precision: 0.5000396825396825, Recall: 0.6766666666666667, f1: 0.5602319902319903\n",
      "Precision: 0.5275, Recall: 0.47000000000000003, f1: 0.4561627261627262\n",
      "Precision: 0.42444444444444446, Recall: 0.42666666666666664, f1: 0.40921356421356425\n",
      "Precision: 0.5001587301587301, Recall: 0.5033333333333333, f1: 0.47556943056943063\n",
      "Precision: 0.48908730158730157, Recall: 0.56, f1: 0.5094194694194694\n",
      "Precision: 0.45, Recall: 0.13333333333333336, f1: 0.19523809523809527\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "algo_pool = [knn,nc,tree,log,svm,bag,rf,ada,gb,gauss]\n",
    "\n",
    "for i in algo_pool:\n",
    "    f1 = np.mean(cross_val_score(estimator=i,X=metadata[0:95],y=movies[0:95].loc[:,'goodforairplane'],\n",
    "                         scoring='f1',cv=10,n_jobs=-1))\n",
    "    precision = np.mean(cross_val_score(estimator=i,X=metadata[0:95],y=movies[0:95].loc[:,'goodforairplane'],\n",
    "                         scoring='precision',cv=10,n_jobs=-1))\n",
    "    recall = np.mean(cross_val_score(estimator=i,X=metadata[0:95],y=movies[0:95].loc[:,'goodforairplane'],\n",
    "                         scoring='recall',cv=10,n_jobs=-1))\n",
    "    print(f'Precision: {precision}, Recall: {recall}, f1: {f1}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Precision: 0.6058333333333332, Recall: 0.6566666666666665, f1: 0.6193706293706293\n",
      "Precision: 0.5913095238095238, Recall: 0.6133333333333334, f1: 0.5925019425019424\n",
      "Precision: 0.4910714285714285, Recall: 0.53, f1: 0.479965034965035\n",
      "Precision: 0.5650793650793652, Recall: 0.5833333333333334, f1: 0.5491286491286491\n",
      "Precision: 0.5140873015873015, Recall: 0.73, f1: 0.6006327006327006\n",
      "Precision: 0.5066666666666666, Recall: 0.45000000000000007, f1: 0.457965367965368\n",
      "Precision: 0.5066666666666666, Recall: 0.48, f1: 0.48386169386169386\n",
      "Precision: 0.46464285714285714, Recall: 0.45, f1: 0.44125485625485633\n",
      "Precision: 0.4966666666666667, Recall: 0.48999999999999994, f1: 0.47472027972027975\n",
      "Precision: 0.5, Recall: 0.17, f1: 0.23968253968253966\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for i in algo_pool:\n",
    "    f1 = np.mean(cross_val_score(estimator=i,X=meta_and_user[0:95],y=movies[0:95].loc[:,'goodforairplane'],\n",
    "                         scoring='f1',cv=10,n_jobs=-1))\n",
    "    precision = np.mean(cross_val_score(estimator=i,X=meta_and_user[0:95],y=movies[0:95].loc[:,'goodforairplane'],\n",
    "                         scoring='precision',cv=10,n_jobs=-1))\n",
    "    recall = np.mean(cross_val_score(estimator=i,X=meta_and_user[0:95],y=movies[0:95].loc[:,'goodforairplane'],\n",
    "                         scoring='recall',cv=10,n_jobs=-1))\n",
    "    print(f'Precision: {precision}, Recall: {recall}, f1: {f1}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import LVW\n",
    "\n",
    "metadata_selected_features = LVW.lvw(metadata[0:95],movies[0:95].loc[:,'goodforairplane'],74,0.)\n",
    "meta_and_user_selected_fetures = LVW.lvw(meta_and_user[0:95],movies[0:95].loc[:,'goodforairplane'],74,0.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meta_algo_pool = [knn,nc,tree,log,svm,bag,rf,ada,gb]\n",
    "text_algo_pool = [gauss,knn,svm]\n",
    "visual_algo_pool = [knn,tree,log,svm,rf,ada,gb]\n",
    "audio_algo_pool = [log,gb]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in meta_algo_pool:\n",
    "    f1 = np.mean(cross_val_score(estimator=i,X=metadata[0:95].loc[metadata_selected_features],y=movies[0:95].loc[:,'goodforairplane'],\n",
    "                         scoring='f1',cv=10,n_jobs=-1))\n",
    "    precision = np.mean(cross_val_score(estimator=i,X=metadata[0:95].loc[metadata_selected_features],y=movies[0:95].loc[:,'goodforairplane'],\n",
    "                         scoring='precision',cv=10,n_jobs=-1))\n",
    "    recall = np.mean(cross_val_score(estimator=i,X=metadata[0:95].loc[metadata_selected_features],y=movies[0:95].loc[:,'goodforairplane'],\n",
    "                         scoring='recall',cv=10,n_jobs=-1))\n",
    "    print(f'Precision: {precision}, Recall: {recall}, f1: {f1}')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn_meta = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski',\n",
    "                           metric_params=None,n_jobs=-1)\n",
    "\n",
    "nc_meta = NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
    "\n",
    "tree_meta = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                              min_weight_fraction_leaf=0.0, max_features=None, random_state=0, max_leaf_nodes=None,\n",
    "                              class_weight=None, presort=False)\n",
    "\n",
    "log_meta = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1,\n",
    "                         class_weight=None, random_state=0, solver='liblinear', max_iter=100, multi_class='ovr', \n",
    "                         verbose=0, warm_start=False, n_jobs=-1)\n",
    "\n",
    "svm_meta = SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, \n",
    "          cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=None, random_state=0)\n",
    "\n",
    "bag_meta = BaggingClassifier(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, \n",
    "                        bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=-1, random_state=0, verbose=0)\n",
    "\n",
    "rf_meta = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "                            min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, bootstrap=True, \n",
    "                            oob_score=False, n_jobs=-1, random_state=0, verbose=0, warm_start=False, class_weight=None)\n",
    "\n",
    "ada_meta = AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=0)\n",
    "\n",
    "gb_meta = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, min_samples_split=2,\n",
    "                                min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, init=None, random_state=0,\n",
    "                                max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto')\n",
    "\n",
    "gauss_text = GaussianNB()\n",
    "\n",
    "knn_text = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski',\n",
    "                           metric_params=None,n_jobs=-1)\n",
    "\n",
    "svm_text = SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, \n",
    "          cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=None, random_state=0)\n",
    "\n",
    "knn_vis = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski',\n",
    "                           metric_params=None,n_jobs=-1)\n",
    "\n",
    "tree_vis = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                              min_weight_fraction_leaf=0.0, max_features=None, random_state=0, max_leaf_nodes=None,\n",
    "                              class_weight=None, presort=False)\n",
    "\n",
    "log_vis = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1,\n",
    "                         class_weight=None, random_state=0, solver='liblinear', max_iter=100, multi_class='ovr', \n",
    "                         verbose=0, warm_start=False, n_jobs=-1)\n",
    "\n",
    "svm_vis = SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, \n",
    "          cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=None, random_state=0)\n",
    "\n",
    "rf_vis = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "                            min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, bootstrap=True, \n",
    "                            oob_score=False, n_jobs=-1, random_state=0, verbose=0, warm_start=False, class_weight=None)\n",
    "\n",
    "ada_vis = AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=0)\n",
    "\n",
    "gb_vis = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, min_samples_split=2,\n",
    "                                min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, init=None, random_state=0,\n",
    "                                max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto')\n",
    "\n",
    "log_audio = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1,\n",
    "                         class_weight=None, random_state=0, solver='liblinear', max_iter=100, multi_class='ovr', \n",
    "                         verbose=0, warm_start=False, n_jobs=-1)\n",
    "\n",
    "gb_audio = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, min_samples_split=2,\n",
    "                                min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, init=None, random_state=0,\n",
    "                                max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto')\n",
    "\n",
    "estimators = [('knn_meta',knn_meta),('nc_meta',nc_meta),('tree_meta',tree_meta),('log_meta',log_meta),('svm_meta',svm_meta),\n",
    "              ('bag_meta',bag_meta),('rf_meta',rf_meta),('ada_meta',ada_meta),('gb_meta',gb_meta),('gauss_text',gauss_text),\n",
    "              ('knn_text',knn_text),('svm_text',svm_text),('knn_vis',knn_vis),('tree_vis',tree_vis),('log_vis',log_vis),\n",
    "              ('svm_vis',svm_vis),('rf_vis',rf_vis),('ada_vis',ada_vis),('gb_vis',gb_vis),('log_audio',log_audio),('gb_audio',gb_audio)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from stacking_classifiers import stacking_classifier_performance_cv\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    res = stacking_classifier_performance_cv(estimators,metadata,metadata,metadata,metadata,movies[0:95].loc[:,'goodforairplane'])\n",
    "    print(res)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}